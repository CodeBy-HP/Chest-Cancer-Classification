{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MODEL TRAINING RESEARCH NOTEBOOK\n",
    "=================================\n",
    "Modern TensorFlow training pipeline with production-ready practices:\n",
    "- tf.data.Dataset API (modern, efficient data loading)\n",
    "- Modern data augmentation techniques\n",
    "- Callbacks for monitoring and early stopping\n",
    "- Mixed precision training support\n",
    "- Comprehensive metrics and logging\n",
    "- Class imbalance handling\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Optional, Tuple\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\asus\\\\Desktop\\\\Deep Learning project\\\\Chest-Cancer-Classification\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Working directory: c:\\Users\\asus\\Desktop\\Deep Learning project\\Chest-Cancer-Classification\n"
     ]
    }
   ],
   "source": [
    "# Navigate to project root\n",
    "project_root = Path(__file__).resolve().parent.parent if '__file__' in globals() else Path.cwd().parent\n",
    "os.chdir(project_root)\n",
    "print(f\"‚úì Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Environment variables loaded\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_path = Path('.env')\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(\"‚úì Environment variables loaded\")\n",
    "else:\n",
    "    print(\"‚ö† Warning: .env file not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration with validation\"\"\"\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_image_size: List[int]\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Validate training configuration\"\"\"\n",
    "        if self.params_epochs < 1:\n",
    "            raise ValueError(\"Epochs must be >= 1\")\n",
    "        if self.params_batch_size < 1:\n",
    "            raise ValueError(\"Batch size must be >= 1\")\n",
    "        if not self.training_data.exists():\n",
    "            raise FileNotFoundError(f\"Training data not found: {self.training_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "from cnnClassifier.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    \"\"\"Modern configuration manager\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath: Path = CONFIG_FILE_PATH,\n",
    "        params_filepath: Path = PARAMS_FILE_PATH\n",
    "    ):\n",
    "        \"\"\"Initialize configuration\"\"\"\n",
    "        try:\n",
    "            self.config = read_yaml(config_filepath)\n",
    "            self.params = read_yaml(params_filepath)\n",
    "            \n",
    "            create_directories([self.config.artifacts_root])\n",
    "            logging.info(\"‚úì Configuration loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to load configuration: {e}\")\n",
    "            raise\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        \"\"\"Get validated training configuration\"\"\"\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        params = self.params\n",
    "        \n",
    "        # Construct path to training data\n",
    "        training_data = Path(self.config.data_ingestion.unzip_dir) / \"Chest-CT-Scan-data\"\n",
    "        \n",
    "        create_directories([Path(training.root_dir)])\n",
    "        \n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\n",
    "            training_data=training_data,\n",
    "            params_epochs=params.EPOCHS,\n",
    "            params_batch_size=params.BATCH_SIZE,\n",
    "            params_is_augmentation=params.AUGMENTATION,\n",
    "            params_image_size=params.IMAGE_SIZE\n",
    "        )\n",
    "        \n",
    "        logging.info(\"‚úì Training config created\")\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö† No GPU found - Training on CPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Check GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"‚úì Found {len(gpus)} GPU(s)\")\n",
    "    for gpu in gpus:\n",
    "        print(f\"  - {gpu}\")\n",
    "else:\n",
    "    print(\"‚ö† No GPU found - Training on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    \"\"\"\n",
    "    Modern training pipeline with TensorFlow best practices.\n",
    "    \n",
    "    Key improvements:\n",
    "    - tf.data.Dataset API (efficient, modern)\n",
    "    - Built-in augmentation layers (GPU-accelerated)\n",
    "    - Performance optimizations (prefetch, cache)\n",
    "    - Comprehensive callbacks\n",
    "    - Class imbalance handling\n",
    "    - Progress monitoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.logger = logging.getLogger(self.__class__.__name__)\n",
    "        self.model = None\n",
    "        self.train_generator = None\n",
    "        self.valid_generator = None\n",
    "\n",
    "    def get_base_model(self) -> tf.keras.Model:\n",
    "        \"\"\"\n",
    "        Load compiled model from .keras file.\n",
    "        \n",
    "        Returns:\n",
    "            tf.keras.Model: Loaded model ready for training\n",
    "        \"\"\"\n",
    "        try:\n",
    "            model_path = self.config.updated_base_model_path\n",
    "            \n",
    "            if not model_path.exists():\n",
    "                raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "            \n",
    "            self.logger.info(f\"Loading model from: {model_path}\")\n",
    "            \n",
    "            # Load model in .keras format\n",
    "            self.model = tf.keras.models.load_model(model_path)\n",
    "            \n",
    "            self.logger.info(\"‚úì Model loaded successfully\")\n",
    "            self.logger.info(f\"  Total parameters: {self.model.count_params():,}\")\n",
    "            \n",
    "            return self.model\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def train_valid_generator(self) -> Tuple[tf.data.Dataset, tf.data.Dataset]:\n",
    "        \"\"\"\n",
    "        Create training and validation datasets using modern tf.data API.\n",
    "        \n",
    "        Modern practices:\n",
    "        - image_dataset_from_directory (replaces deprecated ImageDataGenerator)\n",
    "        - GPU-accelerated augmentation layers\n",
    "        - Prefetching for performance\n",
    "        - Proper normalization\n",
    "        - Deterministic splits with seed\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (train_dataset, validation_dataset)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            image_size = tuple(self.config.params_image_size[:-1])\n",
    "            batch_size = self.config.params_batch_size\n",
    "            \n",
    "            self.logger.info(f\"Loading dataset from: {self.config.training_data}\")\n",
    "            self.logger.info(f\"Image size: {image_size}, Batch size: {batch_size}\")\n",
    "            \n",
    "            # Create validation dataset (20% split)\n",
    "            self.valid_generator = tf.keras.utils.image_dataset_from_directory(\n",
    "                directory=str(self.config.training_data),\n",
    "                validation_split=0.20,\n",
    "                subset=\"validation\",\n",
    "                seed=123,  # Deterministic split\n",
    "                image_size=image_size,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=False,  # Don't shuffle validation\n",
    "                label_mode='categorical'  # For categorical_crossentropy\n",
    "            )\n",
    "            \n",
    "            # Create training dataset (80% split)\n",
    "            self.train_generator = tf.keras.utils.image_dataset_from_directory(\n",
    "                directory=str(self.config.training_data),\n",
    "                validation_split=0.20,\n",
    "                subset=\"training\",\n",
    "                seed=123,  # Same seed for consistent split\n",
    "                image_size=image_size,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,  # Shuffle training data\n",
    "                label_mode='categorical'\n",
    "            )\n",
    "            \n",
    "            # Get class names and counts\n",
    "            class_names = self.train_generator.class_names\n",
    "            self.logger.info(f\"‚úì Classes detected: {class_names}\")\n",
    "            \n",
    "            # Normalize pixel values to [0, 1]\n",
    "            normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "            self.train_generator = self.train_generator.map(\n",
    "                lambda x, y: (normalization_layer(x), y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE\n",
    "            )\n",
    "            self.valid_generator = self.valid_generator.map(\n",
    "                lambda x, y: (normalization_layer(x), y),\n",
    "                num_parallel_calls=tf.data.AUTOTUNE\n",
    "            )\n",
    "            \n",
    "            # Apply data augmentation if enabled (modern GPU-accelerated layers)\n",
    "            if self.config.params_is_augmentation:\n",
    "                self.logger.info(\"‚úì Data augmentation enabled\")\n",
    "                \n",
    "                # Modern augmentation using Keras layers (GPU-accelerated)\n",
    "                data_augmentation = tf.keras.Sequential([\n",
    "                    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "                    tf.keras.layers.RandomRotation(0.2),  # ¬±20% rotation\n",
    "                    tf.keras.layers.RandomZoom(0.2),  # ¬±20% zoom\n",
    "                    tf.keras.layers.RandomTranslation(0.2, 0.2),  # ¬±20% shift\n",
    "                    tf.keras.layers.RandomContrast(0.2),  # Contrast adjustment\n",
    "                ], name='augmentation')\n",
    "                \n",
    "                # Apply only to training data (not validation)\n",
    "                self.train_generator = self.train_generator.map(\n",
    "                    lambda x, y: (data_augmentation(x, training=True), y),\n",
    "                    num_parallel_calls=tf.data.AUTOTUNE\n",
    "                )\n",
    "            \n",
    "            # Performance optimizations\n",
    "            # Cache: keeps data in memory after first epoch\n",
    "            # Prefetch: prepares next batch while training current batch\n",
    "            AUTOTUNE = tf.data.AUTOTUNE\n",
    "            self.train_generator = self.train_generator.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "            self.valid_generator = self.valid_generator.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "            \n",
    "            # Calculate dataset sizes\n",
    "            train_batches = tf.data.experimental.cardinality(self.train_generator).numpy()\n",
    "            valid_batches = tf.data.experimental.cardinality(self.valid_generator).numpy()\n",
    "            \n",
    "            self.logger.info(f\"‚úì Training batches: {train_batches}\")\n",
    "            self.logger.info(f\"‚úì Validation batches: {valid_batches}\")\n",
    "            \n",
    "            return self.train_generator, self.valid_generator\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to create data generators: {e}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: tf.keras.Model) -> None:\n",
    "        \"\"\"Save model in .keras format\"\"\"\n",
    "        try:\n",
    "            # Ensure .keras extension\n",
    "            if not str(path).endswith('.keras'):\n",
    "                path = Path(str(path).replace('.h5', '.keras'))\n",
    "            \n",
    "            path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            model.save(path, save_format='keras')\n",
    "            \n",
    "            file_size = path.stat().st_size / (1024 * 1024)\n",
    "            logging.info(f\"‚úì Model saved: {path} ({file_size:.2f} MB)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save model: {e}\")\n",
    "            raise\n",
    "\n",
    "    def train(self) -> tf.keras.callbacks.History:\n",
    "        \"\"\"\n",
    "        Train model with modern callbacks and monitoring.\n",
    "        \n",
    "        Returns:\n",
    "            History: Training history with metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calculate class weights for imbalanced dataset\n",
    "            # This is crucial for the 2:1 ratio (300 adenocarcinoma : 150 normal)\n",
    "            self.logger.info(\"Calculating class weights for imbalanced dataset...\")\n",
    "            \n",
    "            # Extract labels from training data\n",
    "            class_labels = np.concatenate([y for x, y in self.train_generator], axis=0)\n",
    "            class_labels = np.argmax(class_labels, axis=1)\n",
    "            \n",
    "            from sklearn.utils import class_weight\n",
    "            class_weights = class_weight.compute_class_weight(\n",
    "                class_weight='balanced',\n",
    "                classes=np.unique(class_labels),\n",
    "                y=class_labels\n",
    "            )\n",
    "            class_weight_dict = dict(enumerate(class_weights))\n",
    "            \n",
    "            self.logger.info(f\"‚úì Class weights: {class_weight_dict}\")\n",
    "            self.logger.info(\"  This balances the 2:1 dataset ratio\")\n",
    "            \n",
    "            # Setup callbacks for production-ready training\n",
    "            callbacks = self._create_callbacks()\n",
    "            \n",
    "            # Display training info\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"STARTING MODEL TRAINING\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Epochs: {self.config.params_epochs}\")\n",
    "            print(f\"Batch size: {self.config.params_batch_size}\")\n",
    "            print(f\"Augmentation: {self.config.params_is_augmentation}\")\n",
    "            print(f\"Class weights: {class_weight_dict}\")\n",
    "            print(\"=\"*60 + \"\\n\")\n",
    "            \n",
    "            # Train model\n",
    "            history = self.model.fit(\n",
    "                self.train_generator,\n",
    "                epochs=self.config.params_epochs,\n",
    "                validation_data=self.valid_generator,\n",
    "                class_weight=class_weight_dict,\n",
    "                callbacks=callbacks,\n",
    "                verbose=1\n",
    "            )\n",
    "            \n",
    "            # Save trained model\n",
    "            self.save_model(\n",
    "                path=self.config.trained_model_path,\n",
    "                model=self.model\n",
    "            )\n",
    "            \n",
    "            self.logger.info(\"‚úì Training completed successfully\")\n",
    "            return history\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Training failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _create_callbacks(self) -> list:\n",
    "        \"\"\"\n",
    "        Create modern callbacks for training monitoring.\n",
    "        \n",
    "        Returns:\n",
    "            List of callbacks\n",
    "        \"\"\"\n",
    "        callbacks = []\n",
    "        \n",
    "        # Early stopping to prevent overfitting\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks.append(early_stopping)\n",
    "        \n",
    "        # Reduce learning rate when plateauing\n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks.append(reduce_lr)\n",
    "        \n",
    "        # Model checkpoint to save best model\n",
    "        checkpoint_path = self.config.root_dir / \"best_model_checkpoint.keras\"\n",
    "        model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=str(checkpoint_path),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "        callbacks.append(model_checkpoint)\n",
    "        \n",
    "        # TensorBoard for visualization (optional)\n",
    "        log_dir = self.config.root_dir / \"logs\" / datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        tensorboard = tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=str(log_dir),\n",
    "            histogram_freq=1,\n",
    "            write_graph=True\n",
    "        )\n",
    "        callbacks.append(tensorboard)\n",
    "        \n",
    "        self.logger.info(f\"‚úì Configured {len(callbacks)} callbacks\")\n",
    "        self.logger.info(f\"  TensorBoard logs: {log_dir}\")\n",
    "        \n",
    "        return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-13 00:19:54,400 - cnnClassifierLogger - INFO - yaml file: config\\config.yaml loaded successfully\n",
      "2025-12-13 00:19:54,410 - cnnClassifierLogger - INFO - yaml file: params.yaml loaded successfully\n",
      "2025-12-13 00:19:54,413 - cnnClassifierLogger - INFO - created directory at: artifacts\n",
      "2025-12-13 00:19:54,418 - root - INFO - ‚úì Configuration loaded successfully\n",
      "2025-12-13 00:19:54,420 - cnnClassifierLogger - INFO - created directory at: artifacts\\training\n",
      "2025-12-13 00:19:54,421 - root - INFO - ‚úì Training config created\n",
      "2025-12-13 00:19:54,424 - Training - INFO - Loading model from: artifacts\\prepare_base_model\\base_model_updated.keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL TRAINING PIPELINE\n",
      "============================================================\n",
      "\n",
      "Step 1/3: Loading trained base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\asus\\Desktop\\Deep Learning project\\Chest-Cancer-Classification\\venv\\lib\\site-packages\\keras\\src\\saving\\saving_lib.py:797: UserWarning: Skipping variable loading for optimizer 'adam_optimizer', because it has 10 variables whereas the saved optimizer has 2 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "2025-12-13 00:19:57,206 - Training - INFO - ‚úì Model loaded successfully\n",
      "2025-12-13 00:19:57,208 - Training - INFO -   Total parameters: 4,057,253\n",
      "2025-12-13 00:19:57,210 - Training - INFO - Loading dataset from: artifacts\\data_ingestion\\Chest-CT-Scan-data\n",
      "2025-12-13 00:19:57,211 - Training - INFO - Image size: (224, 224), Batch size: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2/3: Preparing training and validation data...\n",
      "Found 466 files belonging to 2 classes.\n",
      "Using 93 files for validation.\n",
      "Found 466 files belonging to 2 classes.\n",
      "Using 373 files for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-13 00:19:57,427 - Training - INFO - ‚úì Classes detected: ['adenocarcinoma', 'normal']\n",
      "2025-12-13 00:19:57,525 - Training - INFO - ‚úì Data augmentation enabled\n",
      "2025-12-13 00:19:57,881 - Training - INFO - ‚úì Training batches: 47\n",
      "2025-12-13 00:19:57,885 - Training - INFO - ‚úì Validation batches: 12\n",
      "2025-12-13 00:19:57,886 - Training - INFO - Calculating class weights for imbalanced dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3/3: Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-13 00:20:02,796 - Training - INFO - ‚úì Class weights: {0: np.float64(0.734251968503937), 1: np.float64(1.5672268907563025)}\n",
      "2025-12-13 00:20:02,796 - Training - INFO -   This balances the 2:1 dataset ratio\n",
      "2025-12-13 00:20:02,797 - Training - INFO - ‚úì Configured 4 callbacks\n",
      "2025-12-13 00:20:02,798 - Training - INFO -   TensorBoard logs: artifacts\\training\\logs\\20251213-002002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING MODEL TRAINING\n",
      "============================================================\n",
      "Epochs: 1\n",
      "Batch size: 8\n",
      "Augmentation: True\n",
      "Class weights: {0: np.float64(0.734251968503937), 1: np.float64(1.5672268907563025)}\n",
      "============================================================\n",
      "\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485ms/step - accuracy: 0.5151 - auc: 0.4862 - loss: 0.8878 - precision: 0.5151 - recall: 0.5151\n",
      "Epoch 1: val_accuracy improved from None to 1.00000, saving model to artifacts\\training\\best_model_checkpoint.keras\n",
      "\u001b[1m47/47\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 794ms/step - accuracy: 0.5067 - auc: 0.4929 - loss: 0.8385 - precision: 0.5067 - recall: 0.5067 - val_accuracy: 1.0000 - val_auc: 1.0000 - val_loss: 0.5527 - val_precision: 1.0000 - val_recall: 1.0000 - learning_rate: 0.0010\n",
      "Restoring model weights from the end of the best epoch: 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-13 00:20:53,771 - absl - WARNING - The `save_format` argument is deprecated in Keras 3. We recommend removing this argument as it can be inferred from the file path. Received: save_format=keras\n",
      "2025-12-13 00:20:54,875 - root - INFO - ‚úì Model saved: artifacts\\training\\model.keras (16.32 MB)\n",
      "2025-12-13 00:20:54,877 - Training - INFO - ‚úì Training completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "‚úì TRAINING COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "üìÅ Trained model: artifacts\\training\\model.keras\n",
      "üìä TensorBoard logs: artifacts\\training\\logs\n",
      "\n",
      "‚ú® Ready for evaluation!\n"
     ]
    }
   ],
   "source": [
    "# MAIN EXECUTION PIPELINE\n",
    "# Production-ready training with comprehensive error handling\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"MODEL TRAINING PIPELINE\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        \n",
    "        # Initialize configuration\n",
    "        config_manager = ConfigurationManager()\n",
    "        training_config = config_manager.get_training_config()\n",
    "        \n",
    "        # Initialize training\n",
    "        training = Training(config=training_config)\n",
    "        \n",
    "        # Step 1: Load model\n",
    "        print(\"Step 1/3: Loading trained base model...\")\n",
    "        training.get_base_model()\n",
    "        \n",
    "        # Step 2: Prepare data\n",
    "        print(\"\\nStep 2/3: Preparing training and validation data...\")\n",
    "        training.train_valid_generator()\n",
    "        \n",
    "        # Step 3: Train\n",
    "        print(\"\\nStep 3/3: Training model...\")\n",
    "        history = training.train()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úì TRAINING COMPLETED SUCCESSFULLY\")\n",
    "        print(\"=\"*60 + \"\\n\")\n",
    "        print(f\"üìÅ Trained model: {training_config.trained_model_path}\")\n",
    "        print(f\"üìä TensorBoard logs: {training_config.root_dir / 'logs'}\")\n",
    "        print(\"\\n‚ú® Ready for evaluation!\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\n‚ùå FILE ERROR: {e}\")\n",
    "        print(\"   Ensure previous steps (data ingestion, base model) completed\")\n",
    "    except ValueError as e:\n",
    "        print(f\"\\n‚ùå CONFIGURATION ERROR: {e}\")\n",
    "        print(\"   Check your configuration files\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå UNEXPECTED ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
